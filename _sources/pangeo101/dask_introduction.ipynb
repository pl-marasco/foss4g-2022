{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bfbae7a-12f1-4787-a520-c3de7529168d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Parallel computing with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281bb79b-1b06-42c9-98d5-6185252c86e5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Authors & Contributors\n",
    "### Authors\n",
    "- Tina Odaka, Ifremer (France), [@tinaok](https://github.com/tinaok)\n",
    "- Pier Lorenzo Marasco, Ispra (Italy), [@pl-marasco](https://github.com/pl-marasco)\n",
    "\n",
    "### Contributors\n",
    "- Anne Fouilloux, University of Oslo (Norway), @annefou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f245decb-8706-4b55-aead-79dd7a621bdd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<i class=\"fa-question-circle fa\" style=\"font-size: 22px;color:#666;\"></i> Overview\n",
    "    <br>\n",
    "    <br>\n",
    "    <b>Questions</b>\n",
    "    <ul>\n",
    "        <li>What is Dask?</li>\n",
    "        <li>How can I parallelize my data analysis with Dask?</li>\n",
    "    </ul>\n",
    "    <b>Objectives</b>\n",
    "    <ul>\n",
    "        <li>Learn about Dask</li>\n",
    "        <li>Learn about Dask Gateway, Dask client, scheduler, workers</li>\n",
    "        <li>Understand out-of-core and speed-up limitations</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c3e5a-1ddf-4178-a05e-2ce711ab1b8b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Context\n",
    "\n",
    "\n",
    "We will be using [Dask](https://docs.dask.org/) with [Xarray](https://docs.xarray.dev/en/stable/) to parallelize our data analysis. The analysis is very similar to what we have done in previous episodes but this time we will use data on a global coverage that we read from a shared catalog (stored online in the Pangeo EOSC Openstack Object Storage).\n",
    "\n",
    "### Data\n",
    "\n",
    "In this episode, we will be using Global Long Term Statistics (1999-2019) product provided by the [Copernicus Global Land Service over Lombardia](https://land.copernicus.eu/global/index.html) and access them through [S3-comptabile storage](https://en.wikipedia.org/wiki/Amazon_S3) ([OpenStack Object Storage \"Swift\"](https://wiki.openstack.org/wiki/Swift)) with a data catalog we have created and made publicly available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c600794-dd9e-400b-bd09-dbb6e7039dad",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "This episode uses the following Python packages:\n",
    "\n",
    "- pooch {cite:ps}`e-pooch-Uieda2020`\n",
    "- s3fs {cite:ps}`e-s3fs-2016`\n",
    "- xarray {cite:ps}`e-xarray-hoyer2017` with [`netCDF4`](https://pypi.org/project/h5netcdf/) and [`h5netcdf`](https://pypi.org/project/h5netcdf/) engines\n",
    "- hvplot {cite:ps}`e-holoviews-rudiger2020`\n",
    "- dask {cite:ps}`e-dask-2016`\n",
    "- graphviz {cite:ps}`e-graphviz-Ellson2003`\n",
    "- numpy {cite:ps}`e-numpy-harris2020`\n",
    "- pandas {cite:ps}`e-pandas-reback2020`\n",
    "- geopandas {cite:ps}`e-geopandas-jordahl2020`\n",
    "\n",
    "Please install these packages if not already available in your Python environment (you might want to take a look at [the Setup page of the tutorial](https://pangeo-data.github.io/foss4g-2022/before/setup.html)).\n",
    "### Packages\n",
    "\n",
    "In this episode, Python packages are imported when we start to use them. However, for best software practices, we recommend you to install and import all the necessary libraries at the top of your Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cd73e3-0120-4457-b84b-ccd9325a72a6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Parallelize with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35db696-501a-4110-a7fa-fafe92dfc0a2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We know from previous chapter [chunking_introduction](./chunking_introduction.ipynb) that chunking is key for analyzing large datasets.  In this episode, we will learn to parallelize our data analysis using chunk and [Dask](https://docs.dask.org/) . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2f3544-8b08-462d-b8a4-c6c0c2f68a26",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### What is [Dask](https://docs.dask.org/) ?\n",
    "\n",
    "**Dask** accelerates the existing Python ecosystem: with very or no changes in your code, you can speed-up computation using Dask.\n",
    "\n",
    "- Dask is a flexible library for parallel computing in Python.\n",
    "- It is widely used for getting the necessary performance when handling large and complex Earth Science datasets.\n",
    "- Dask is powerful, scalable and flexible. It is the leading platform today for analytics.\n",
    "- It scales natively to clusters, cloud, HPC and bridges prototyping up to production.\n",
    "- The strength of Dask is that is accelerates the existing Python ecosystem e.g. Numpy, Pandas and Scikit-learn with few effort from end-users.\n",
    "\n",
    "It is interesting to note that at first, Dask has been created to handle data that is larger than memory, on a single computer. It then was extended with Distributed to compute data in parallel over cluster of computers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e44c561-da09-4051-b562-bca3c276659e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### How does Dask accelerate your data analysis?\n",
    "\n",
    "[Dask have different possibilities to parallelise your computation](https://docs.dask.org/en/stable/10-minutes-to-dask.html).  In this dask_introduction section, we will focus on [Dask Array](https://docs.dask.org/en/stable/array.html) which is widely used in pangeo ecosystem as a back end of Xarray.\n",
    "As shown in the [previous section](./chunking_introduction.ipynb) Dask Array is based on chunks.\n",
    "Chunks of Dask Array is based on many numpy arrays.  By transforming our big datasets to Dask Array, and make use of chunk, a large numpy array is transformed into smaller ones and we can compute each chunk independently.\n",
    "\n",
    "![Dask and Numpy](https://examples.dask.org/_images/dask-array-black-text.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e595d939-199f-4cbd-8107-2b4480a993ac",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Attention</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>`Xarray` uses Dask Arrays instead of Numpy when chunking is enabled, and thus all Xarray operations are distributed through Dask. </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b9fd40-08f4-43e2-897f-b1e745fd215a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### How does Xarray with dask distribute data analysis?\n",
    "\n",
    "\n",
    "When we use chunks with `Xarray`, the real computation is only done when needed; for instance when invoking `compute()` function. Dask generates a **task graph** describing the computation to be done and a **scheduler** executes these tasks across several **workers**.\n",
    "\n",
    "![Xarray with dask](../figures/dask-xarray-explained.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a4ff9d-b390-49f9-a0e6-622170d955af",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ":::{tip}\n",
    "A Dask client can also be created on a single machine (for instance your laptop) e.g. there is no need to have dedicated computational resources. However, speedup will only be limited to your single machine resources if you do not have dedicated computational resources!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27e06ec-f40a-4ed4-b0b0-61db5232e115",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Set up a local Dask \n",
    "\n",
    "There are different methods to use Dask depending on the underlying infrastructure. For this workshop according to the Pangeo EOSC deployment, you will learn how to set up Dask gateways to manage Dask clusters and run our data analysis in parallel e.g. distribute tasks across several workers.\n",
    "\n",
    "However, you do not always need to access a multi-node Dask cluster. It is very handy to prototype and/or run data analysis on your own laptop, or a small server. Let's keep it simple for now and learn how to create a local dask cluster to distribute the work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa52be3e-3924-4d0b-9a46-abd9009e2a3b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create a local dask cluster\n",
    " \n",
    "The Dask client is what allows you to interact with Dask. \n",
    "The Client will create the Directed Acyclic Graph (DAG) of tasks by analysing the code, and will be responsible for telling the scheduler what to compute. It will also gather results from the workers and aggregates the results in the Client process.\n",
    "\n",
    "With no argument to `Client()` function, you create a local dask cluster with a number of workers and threads per worker corresponding to the number of cores in the local machine. Here, we are running this notebook in the cloud, so the number of cores is the number of cores on the cloud computing resource (not on your laptop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879754c-8ede-43ed-86ef-05cd9c288e73",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "\n",
    "client = Client()   # create a local dask cluster on the local machine.\n",
    "client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934eb7fa-76b6-4a22-a139-3eb53989645d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Inspecting the `Cluster Info` section gives us information about the created cluster: we have 4 workers and a total of 4 threads (e.g. 1 thread per worker). you can use `n_workers` and `threads_per_worker` whenvever you want to creat a local dask cluster with less workers and threads than on the local machine. For instance, we could use `n_workers=2` and `threads_per_worker=2` (the total number of threads would still be 4 but each worker would have 2 threads). This is sometimes preferable (in terms of performance) but out of scope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86a4ec6-bbf8-45c5-92d3-4c2ff2b0d119",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Attention</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>Dask will try to hold data on the memory, then try to spill that to hard disk of worker.  If you would like to avoid that dask worker use your local disk ( it will slow down your computation), you can use following command.\n",
    "        <li>import dask.distributed</li>\n",
    "        <li>dask.config.set({\"distributed.worker.memory.spill\": 0}) </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1171f9b8-eb49-43ee-9d3f-5b8d74434e1f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Open kerchunk catalogue, Select a single location and visualize the task graph e\n",
    "\n",
    "Lets open dataset from catalogue we made before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d552c-d986-465c-ab72-578432d579c3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "catalogue=\"https://object-store.cloud.muni.cz/swift/v1/foss4g-catalogue/c_gls_NDVI-LTS_1999-2019.json\"\n",
    "LTS = xr.open_mfdataset(\n",
    "    \"reference://\", engine=\"zarr\",\n",
    "    backend_kwargs={\n",
    "        \"storage_options\": {\n",
    "            \"fo\":catalogue\n",
    "                    },\n",
    "        \"consolidated\": False\n",
    "    }\n",
    ")\n",
    "LTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c7190e-5596-413c-a066-cfe214e933f1",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save=LTS.sel(lat=45.50, lon=9.36, method='nearest')['min'].mean()\n",
    "save.data#.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08b90d-22d6-404f-8c3d-3c557410d601",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As you look at 'task count' we see 6121 task for just selecting one data.  \n",
    "To avoid unecessary operations, we optimize the task graph using `optimize`, and verify the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77254f5a-7001-4fcd-a88e-af4e9a6d96fd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Optimize the task graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c6888b-de87-4989-8975-50a0d2a1fcbe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f28263-a764-4bb2-b1a5-c851ddbaaa86",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(save,) = dask.optimize(save)\n",
    "save.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537cd461-8f9d-4651-9190-73d5eb6a40ef",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now our task is reduced to 73. Lets try to visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4395eadb-769e-4d49-9b75-dbaf0e1360ba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save.data.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3404ea7f-ec0f-412b-8adc-2b681ba75ab4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0698f61f-32bf-4ccc-bfcf-2f58de316431",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Compute on the dask workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45148205-16b9-42a4-aaba-938c5cea593c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d4e8f1-fc6b-4c6f-b7bb-b7a61a3d944d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Close client to terminate local dask cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c8d36a-a4cd-434c-a1bf-cc1850d2a15a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The client will be automatically closed when your Python session ends. When using Jupyter notebooks, we recommend to close it explicitely whenever you are done with your local dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f11f2cf-7418-43de-9b45-847556474b08",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4993fc20-30ba-49d5-9733-240adf90ecaf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Set up Dask Gateway\n",
    "\n",
    "When we want to scale out our data analysis and cannot only use the local machine, we need to be able to access a multi-node Dask cluster. \n",
    "On the EOSC Pangeo infrastructure, we can use Dask gateways to manage Dask clusters and run our data analysis in parallel e.g. distribute tasks across several workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab00293-fc51-409b-8bb6-10a6e04c7249",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway # , BasicAuth\n",
    "gateway = Gateway(\n",
    "#    \"http://api-daskhub-dask-gateway.daskhub:8000/\",\n",
    "#    auth = BasicAuth(password=\"pangeo_dask\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c980455f-e639-47e9-8d53-5f47b00370d2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create a new Dask cluster on the Dask gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4624b389-1add-4476-8aa2-58087c409571",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "cluster = gateway.new_cluster()\n",
    "cluster.scale(1)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d3742e-2d4b-48a3-a2da-ee67d019dc51",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Get a client from the Dask Gateway Cluster\n",
    "\n",
    "The Dask client is what allows you to interact with Dask. The Client will create the Directed Acyclic Graph (DAG) of tasks by analysing the code, and will be responsible for telling the scheduler what to compute. It will also gather results from the workers and aggregates the results in the Client process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79047f2-044a-41c3-b8f0-4acbd121fa03",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cluster = None # This is needed for building the Jupyter Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9f05e7-0827-4f8a-a6d3-a8c8abfbf42d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "\n",
    "if cluster:\n",
    "    client = Client(cluster) # create a dask Gateway cluster\n",
    "else:\n",
    "    client = Client()   # create a local dask cluster on the machine.\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f4818f-2b6b-44c1-af77-97daf8a1c2a1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Global LTS\n",
    "\n",
    "In the previous episode, we used Long-term Timeseries for the region of Lombardy e.g. a very small area. Now we would like to use the original dataset that has a global coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74e33d9-f980-4416-a3e4-07689d5bf1a8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Read from online kerchunked consolidated dataset\n",
    "\n",
    " We will access Long Term TimeSeries of NDVI statistics from OpenStack Object Storage using the Zarr metadata generated with kerchunk, prepared in [previous chunking_introduction](./chunking_introduction.ipynb) section ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655225ff-99fd-45cb-a31f-606335e39b85",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62abef70-69fb-4611-84b4-40854075d665",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "catalogue=\"https://object-store.cloud.muni.cz/swift/v1/foss4g-catalogue/c_gls_NDVI-LTS_1999-2019.json\"\n",
    "LTS = xr.open_mfdataset(\n",
    "    \"reference://\", engine=\"zarr\",\n",
    "    backend_kwargs={\n",
    "        \"storage_options\": {\n",
    "            \"fo\":catalogue\n",
    "                    },\n",
    "        \"consolidated\": False\n",
    "    }\n",
    ")\n",
    "LTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Visualize LTS statistics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=[15,5])\n",
    "\n",
    "# Fix extent\n",
    "minval = 0.0\n",
    "maxval = 0.9\n",
    "\n",
    "itime=0 # plot the first date\n",
    "\n",
    "# Plot 1 for min subplot argument (nrows, ncols, nplot)\n",
    "# here 1 row, 2 columns and 1st plot\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "LTS.isel(time=itime)['min'].plot(ax=ax1)\n",
    "# Plot 2 for max\n",
    "# 2nd plot\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "LTS.isel(time=itime)['max'].plot(ax=ax2)\n",
    "\n",
    "# Title for both plots\n",
    "fig.suptitle('LTS NDVI statistics (Minimum and Maximum)', fontsize=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fix time coordinate\n",
    "\n",
    "as observed data are coming with a predefined year. To let xarray automatically align the LTS with the lastest NDVI values the time dimension, needs to be shifted to the NDVI values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dates_2022 = pd.date_range('20220101', '20221231')\n",
    "time_list = dates_2022[np.isin(dates_2022.day, [1,11,21])]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LTS = LTS.rename({'INDEX': 'time'}).assign_coords(time=time_list)\n",
    "LTS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clip LTS over Lombardia\n",
    "As in previous episodes, we use a shapefile over Italy to select data over this Area of Interest (AOI)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7f7565-c5bf-4ee8-880b-bcc218c6db5a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e61dab-f88b-4f97-b3c6-ec3f4e0a2ba4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    GAUL = gpd.read_file('Italy.geojson')\n",
    "except:\n",
    "    GAUL = gpd.read_file('zip+https://mars.jrc.ec.europa.eu/asap/files/gaul1_asap.zip') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1955a5cc-1e38-4e1a-bd42-587da11731b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "AOI_name = 'Lombardia'\n",
    "AOI = GAUL[GAUL.name1 == AOI_name]\n",
    "AOI_poly = AOI.geometry\n",
    "AOI_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We first select a geographical area that covers Lombardia (so that we have a first reduction from the global coverage) and then clip using the shapefile to avoid useless pixels."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760227fe-a042-4d05-a23d-5cf0f6a5585d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LTS = LTS.sel(lat=slice(46.5,44.5), lon=slice(8.5,11.5))\n",
    "LTS.rio.write_crs(4326, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0011f8f6-12f2-4b5e-85dd-59c6c1de8fa8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LTS = LTS.rio.clip(AOI_poly, crs=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1ae976-c601-4450-97ea-78050c272919",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f6c8be-448c-43f0-bcac-b40faa02a2c0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "LTS.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e503edd2-cfc7-4f8c-a06e-481e63527ba9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35969623-39a1-4ea2-bf4b-136b2225ce61",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "LTS_min = LTS['min']\n",
    "(LTS_min,)=dask.optimize(LTS_min)\n",
    "LTS_min.data.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4e7b50-1ecd-4898-8350-d6b98f471f90",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "LTS_min.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a801e8ac-137b-4eeb-bb89-280ed8b9bca8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "LTS_max = LTS['max']\n",
    "(LTS_max,)=dask.optimize(LTS_max)\n",
    "LTS_max.data.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165c3bcf-61e0-4262-9efc-a132d81ca0ec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "LTS_max.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18285df-862d-487c-ab2f-57823907a3f1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Get NDVI for 2022 over Lombardia\n",
    "\n",
    "We re-use the file we created during the first episode. If the file is missing it will be downloaded from Zenodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d333a4-228c-4eff-9252-94e13c0a7828",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pooch\n",
    "try:\n",
    "    cgls_ds = xr.open_dataset('C_GLS_NDVI_20220101_20220701_Lombardia_S3_2_masked.nc')\n",
    "except:\n",
    "    cgls_file = pooch.retrieve(\n",
    "        url=\"https://zenodo.org/record/6969999/files/C_GLS_NDVI_20220101_20220701_Lombardia_S3_2_masked.nc\",\n",
    "        known_hash=\"md5:be3f16913ebbdb4e7af227f971007b22\",\n",
    "        path=f\".\",)    \n",
    "    cgls_ds = xr.open_dataset(cgls_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6224f971-6ab1-4db3-992f-a64dce84d04c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cgls_ds.NDVI.rio.write_crs(4326, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d385fbe-26a2-4f67-9ec7-026beb52dfba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NDVI_AOI = NDVI_AOI.rio.clip(AOI_poly, crs=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e19678a-758b-446a-a198-f66849e39085",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NDVI_AOI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107b3d48-a0ed-4ae8-8b7f-48e55823e8ee",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The nominal spatial resolution of the Long term statistics is 1km. As the current NDVI product has a nominal spatial resolution of 300m a re projection is needed. RioXarray through RasterIO that wraps the GDAL method can take care of this. More info about all the options can be found [here](https://rasterio.readthedocs.io/en/stable/api/rasterio.warp.html#rasterio.warp.reproject)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c80a2e8-80ba-44e3-a51b-e8476b465a2c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NDVI_1k = NDVI_AOI.rio.reproject_match(LTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d017d9ac-5d82-4280-9f4c-92ce7622ef56",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NDVI_1k = NDVI_1k.rename({'x': 'lon', 'y':'lat'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b229b0ef-13d1-4443-b5bd-708eaf8b8be1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "VCI = ((NDVI_1k - LTS['min']) / (LTS['max'] - LTS['min'])) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78b857e-7627-4d88-a83d-f3719a7f5391",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "VCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5829a5-4294-4ccf-906b-213344db0c2b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "VCI.name = 'VCI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f6112f-0bd5-41de-8374-547636380ef1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "VCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f36567b-8d2e-4f37-83a6-9f8062d05717",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "VCI_c = VCI.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153bfc7a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from hvplot import xarray\n",
    "VCI.hvplot(x = 'lat', y = 'lon',\n",
    "           cmap='RdYlGn', clim=(-200,+200), alpha=0.7,\n",
    "           geo=True, tiles= 'CartoLight',\n",
    "           title=f'CGLS VCI {AOI_name} {VCI.isel(time=-1).time.dt.date.data}',\n",
    "           width=800, height=700,\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d7df1-b4f8-47e6-bbe3-ad29601ddf13",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now you have catalogue, original data source, both on cloud space, thus even from dask workers which do not have access to your NFS local disk space, data are accessible.\n",
    "Now you are ready to parallelize your analysis using dask workers from dask gateway!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df67cb31-b8f2-4d91-a37f-95d2a4ca328b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1499dae2-f69e-4764-b1fd-fb3aef29e29d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf1f23a-8838-41fe-96d7-4f7b0fb9cc3f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56304d1c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Packages citation\n",
    "\n",
    "```{bibliography}\n",
    ":style: alpha\n",
    ":filter: topic % \"dask\" and topic % \"package\"\n",
    ":keyprefix: e-\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}